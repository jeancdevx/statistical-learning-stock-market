# Proyecto de Aprendizaje Estad√≠stico: Clasificaci√≥n Binaria en el Mercado NYSE

Proyecto acad√©mico de clasificaci√≥n binaria para predecir si el precio de apertura del d√≠a siguiente ser√° mayor que el precio de cierre del d√≠a actual en acciones del NYSE.

## üìã Tabla de Contenidos

- [Descripci√≥n del Proyecto](#descripci√≥n-del-proyecto)
- [Requisitos Previos](#requisitos-previos)
- [Instalaci√≥n](#instalaci√≥n)
- [Obtenci√≥n del Dataset](#obtenci√≥n-del-dataset)
- [Estructura del Proyecto](#estructura-del-proyecto)
- [Ejecuci√≥n del Proyecto](#ejecuci√≥n-del-proyecto)
- [Resultados](#resultados)
- [Detalles T√©cnicos](#detalles-t√©cnicos)

## üìñ Descripci√≥n del Proyecto

Este proyecto implementa la **Secci√≥n 5.1: Aplicaci√≥n al Modelo** del curso de Aprendizaje Estad√≠stico, desarrollando un sistema de clasificaci√≥n binaria para el mercado de valores NYSE.

**Objetivo**: Predecir si `Open_{t+1} > Close_t` utilizando features t√©cnicos derivados de datos hist√≥ricos.

**Modelos implementados**:
- Regresi√≥n Log√≠stica (L2 regularization)
- SVM con kernel RBF

**Protocolo de validaci√≥n**:
- Split temporal: 75% train / 10% validation / 15% test
- Walk-forward cross-validation (k=5) en el conjunto de validaci√≥n
- Evaluaci√≥n √∫nica en el conjunto de test

## üîß Requisitos Previos

- **Python**: 3.8 o superior (proyecto desarrollado con Python 3.13.7)
- **Sistema operativo**: Windows, macOS o Linux
- **Espacio en disco**: ~5 GB (3 GB para datos crudos + 2.8 GB para dataset procesado)
- **RAM**: M√≠nimo 8 GB recomendado para procesar 10M+ registros

## üöÄ Instalaci√≥n

### 1. Clonar el repositorio

```powershell
git clone https://github.com/jeancdevx/statistical-learning-stock-market.git
cd statistical-learning-stock-market
```

### 2. Crear entorno virtual

**En Windows (PowerShell)**:
```powershell
python -m venv venv
.\venv\Scripts\Activate.ps1
```

**En macOS/Linux**:
```bash
python3 -m venv venv
source venv/bin/activate
```

### 3. Instalar dependencias

```powershell
pip install -r requirements.txt
```

Las dependencias principales son:
- `pandas`: Manipulaci√≥n de datos
- `numpy`: Operaciones num√©ricas
- `scikit-learn`: Modelos de ML y m√©tricas
- `matplotlib`: Visualizaciones

## üìä Obtenci√≥n del Dataset

### Opci√≥n 1: Descarga Manual (Recomendada)

Debido a que el sitio de Stooq tiene protecci√≥n CAPTCHA, se requiere descarga manual:

1. **Ir al sitio de Stooq**: https://stooq.com/db/h/

2. **Descargar el archivo**: 
   - Buscar "U.S. stocks - daily (ASCII)"
   - Descargar `d_us_txt.zip` (~500 MB)

3. **Extraer el dataset**:

**En Windows (PowerShell)**:
```powershell
# Crear directorio
New-Item -ItemType Directory -Force -Path datasets\nyse

# Copiar el archivo descargado
Copy-Item Downloads\d_us_txt.zip datasets\nyse\

# Extraer
Expand-Archive -Path datasets\nyse\d_us_txt.zip -DestinationPath datasets\nyse\ -Force
```

**En macOS/Linux**:
```bash
# Crear directorio
mkdir -p datasets/nyse

# Copiar el archivo descargado
cp ~/Downloads/d_us_txt.zip datasets/nyse/

# Extraer
cd datasets/nyse
unzip d_us_txt.zip
cd ../..
```

4. **Verificar la extracci√≥n**:

```powershell
python verificar_dataset.py
```

Deber√≠as ver:
```
Archivos encontrados en NYSE: 3,649
Ejemplo de archivo: datasets/nyse/data/daily/us/nyse stocks/a.us.txt
```

## üìÅ Estructura del Proyecto

```
proyecto/
‚îÇ
‚îú‚îÄ‚îÄ README.md                          # Este archivo
‚îú‚îÄ‚îÄ requirements.txt                   # Dependencias de Python
‚îú‚îÄ‚îÄ .gitignore                        # Archivos excluidos de Git
‚îú‚îÄ‚îÄ verificar_dataset.py              # Script de verificaci√≥n del dataset
‚îÇ
‚îú‚îÄ‚îÄ datasets/                         # Datos (no incluidos en Git)
‚îÇ   ‚îú‚îÄ‚îÄ nyse/                        # Datos crudos de Stooq
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data/daily/us/nyse stocks/  # 3,649 archivos .txt
‚îÇ   ‚îî‚îÄ‚îÄ processed/                   # Datos procesados
‚îÇ       ‚îî‚îÄ‚îÄ dataset_modelado.csv     # Dataset consolidado (2.8 GB)
‚îÇ
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ make_dataset.py          # Construcci√≥n del dataset
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ train_eval.py            # Entrenamiento y evaluaci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ utils/                       # Utilidades (futuro)
‚îÇ
‚îú‚îÄ‚îÄ reports/
‚îÇ   ‚îú‚îÄ‚îÄ metrics/                     # M√©tricas en formato CSV/JSON
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ val_cv_summary_logreg.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ val_cv_summary_svm.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_metrics_logreg.json
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_metrics_svm.json
‚îÇ   ‚îî‚îÄ‚îÄ figures/                     # Visualizaciones
‚îÇ       ‚îú‚îÄ‚îÄ confusion_matrix_logreg.png
‚îÇ       ‚îî‚îÄ‚îÄ confusion_matrix_svm.png
‚îÇ
‚îî‚îÄ‚îÄ app/                             # Aplicaci√≥n (futuro)
```

## ‚ñ∂Ô∏è Ejecuci√≥n del Proyecto

### Paso 1: Construir el Dataset

Este paso transforma los 3,649 archivos individuales de acciones en un √∫nico CSV consolidado con features y labels.

```powershell
python core/data/make_dataset.py
```

**Tiempo estimado**: 15-25 minutos

**Salida esperada**:
```
============================================================
PASO 1: Construcci√≥n del Dataset (5.1)
============================================================

Leyendo archivos desde: datasets/nyse/data/daily/us/nyse stocks
Archivos .txt encontrados: 3,649

Procesando acciones...
  [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 3649/3649

‚úì Acciones v√°lidas: 2,872 (78.7%)
  - Con datos insuficientes: 777

Construyendo features t√©cnicos...
  ‚úì Features construidos: 13 indicadores

Aplicando split temporal (75/10/15)...
  ‚úì Train: 7,779,738 (75.0%)
  ‚úì Val:   1,036,951 (10.0%)
  ‚úì Test:  1,557,855 (15.0%)

Balance de clases:
  Train - Clase 0: 4,017,096 (51.63%) / Clase 1: 3,762,642 (48.37%)
  Val   - Clase 0: 535,531 (51.64%) / Clase 1: 501,420 (48.36%)
  Test  - Clase 0: 757,257 (48.61%) / Clase 1: 800,598 (51.39%)

‚úì Dataset consolidado: 10,374,544 registros
  Guardado en: datasets\processed\dataset_modelado.csv (2832.9 MB)
```

**Features generados** (13 indicadores t√©cnicos):
1. `ret_cc_1`: Retorno close-to-close
2. `ret_oo_1`: Retorno open-to-open
3. `ret_co_1`: Retorno close-to-open
4. `sma_5`: Media m√≥vil simple (5 d√≠as)
5. `sma_10`: Media m√≥vil simple (10 d√≠as)
6. `ema_10`: Media m√≥vil exponencial (10 d√≠as)
7. `mom_5`: Momentum (5 d√≠as)
8. `std_5`: Volatilidad (desviaci√≥n est√°ndar 5 d√≠as)
9. `std_10`: Volatilidad (desviaci√≥n est√°ndar 10 d√≠as)
10. `range_rel`: Rango relativo (High-Low)/Close
11. `vol_ma_10`: Media m√≥vil de volumen (10 d√≠as)
12. `vol_rel`: Volumen relativo
13. `dow`: D√≠a de la semana (1=Lunes, 5=Viernes)

**Target**: `y_{t+1} = 1[Open_{t+1} > Close_t]`

### Paso 2: Entrenar y Evaluar Modelos

Este paso entrena los modelos con walk-forward validation y eval√∫a en el conjunto de test.

```powershell
python core/models/train_eval.py
```

**Tiempo estimado**: 20-35 minutos
- Regresi√≥n Log√≠stica: ~5 minutos
- SVM-RBF: ~20-30 minutos (kernel no lineal es computacionalmente intensivo)

**Proceso**:
1. Carga del dataset (1-2 minutos)
2. Para cada modelo:
   - **Walk-forward validation** (k=5):
     - Fold 1: Entrena en train (75%), valida en val[0:20%]
     - Fold 2: Entrena en train + val[0:20%], valida en val[20:40%]
     - Fold 3: Entrena en train + val[0:40%], valida en val[40:60%]
     - Fold 4: Entrena en train + val[0:60%], valida en val[60:80%]
     - Fold 5: Entrena en train + val[0:80%], valida en val[80:100%]
   - **Test evaluation**: Entrena en train + val (85%), eval√∫a UNA VEZ en test (15%)

**Salida esperada**:
```
============================================================
PASO 3: Entrenamiento y Evaluaci√≥n (5.1)
============================================================

Cargando dataset desde: datasets\processed\dataset_modelado.csv
‚úì Dataset cargado: 10,374,544 registros

Distribuci√≥n:
  Train: 7,779,738 (75.0%)
  Val:   1,036,951 (10.0%)
  Test:  1,557,855 (15.0%)

Features: 13

############################################################
# MODELO: Regresi√≥n Log√≠stica
############################################################

============================================================
Validaci√≥n Walk-Forward: Regresi√≥n Log√≠stica (k=5)
============================================================
  Fold 1/5: acc=0.5234, bacc=0.5234, f1=0.5189, auc=0.5723
  Fold 2/5: acc=0.5241, bacc=0.5240, f1=0.5197, auc=0.5730
  Fold 3/5: acc=0.5238, bacc=0.5237, f1=0.5193, auc=0.5726
  Fold 4/5: acc=0.5235, bacc=0.5234, f1=0.5190, auc=0.5722
  Fold 5/5: acc=0.5232, bacc=0.5231, f1=0.5187, auc=0.5719

Resumen de validaci√≥n:
  Accuracy:          0.5236 ¬± 0.0003
  Balanced Accuracy: 0.5235 ¬± 0.0003
  F1-Score:          0.5191 ¬± 0.0003
  ROC-AUC:           0.5724 ¬± 0.0004

  ‚úì Guardado: reports/metrics/val_cv_summary_logreg.csv

============================================================
Evaluaci√≥n en Test: Regresi√≥n Log√≠stica
============================================================
  Reentrenando en 8,816,689 ejemplos (85%)...
  
  Evaluando en 1,557,855 ejemplos (15%)...
    Accuracy:          0.5240
    Balanced Accuracy: 0.5239
    F1-Score:          0.5265
    ROC-AUC:           0.5732
    Baseline:          0.5139

  Confusion Matrix:
    [[ 394521,  362736]
     [ 379301,  421297]]
    (TN, FP)
    (FN, TP)

  ‚úì Guardado: reports/metrics/test_metrics_logreg.json
  ‚úì Guardado: reports/figures/confusion_matrix_logreg.png

############################################################
# MODELO: SVM-RBF
############################################################

[Similar output for SVM...]

============================================================
¬°Entrenamiento completado!
============================================================
```

## üìà Resultados

Los resultados se guardan en el directorio `reports/`:

### M√©tricas de Validaci√≥n (CSV)

`reports/metrics/val_cv_summary_logreg.csv`:
```csv
fold,accuracy,balanced_accuracy,f1,roc_auc
1,0.5234,0.5234,0.5189,0.5723
2,0.5241,0.5240,0.5197,0.5730
...
```

### M√©tricas de Test (JSON)

`reports/metrics/test_metrics_logreg.json`:
```json
{
  "accuracy": 0.5240,
  "balanced_accuracy": 0.5239,
  "f1": 0.5265,
  "roc_auc": 0.5732,
  "confusion_matrix": [[394521, 362736], [379301, 421297]],
  "baseline_accuracy": 0.5139,
  "n_test": 1557855,
  "n_train": 8816689,
  "validacion": {
    "accuracy_mean": 0.5236,
    "accuracy_std": 0.0003,
    ...
  }
}
```

### Matrices de Confusi√≥n (PNG)

Las matrices de confusi√≥n se guardan como im√°genes en `reports/figures/`:
- `confusion_matrix_logreg.png`
- `confusion_matrix_svm.png`

## üî¨ Detalles T√©cnicos

### Dataset

- **Fuente**: Stooq U.S. Daily (ASCII) - NYSE stocks
- **Per√≠odo**: 1962-01-16 a 2025-10-31 (63 a√±os)
- **Tickers**: 3,649 archivos ‚Üí 2,872 v√°lidos (con datos suficientes)
- **Registros totales**: 10,374,544
- **Tama√±o**: 2.8 GB (CSV)

### Preprocesamiento

1. **Filtrado**: 
   - Eliminaci√≥n de tickers con < 100 registros
   - Eliminaci√≥n de registros con NaN en features

2. **Feature Engineering**:
   - Retornos logar√≠tmicos
   - Medias m√≥viles (SMA, EMA)
   - Indicadores de momentum y volatilidad
   - Features de volumen
   - Protecci√≥n contra divisi√≥n por cero (infinitos ‚Üí NaN)

3. **Split Temporal**:
   - Por ticker para preservar series temporales
   - Train (75%): Primeros 75% de datos de cada ticker
   - Validation (10%): Siguiente 10% de datos
   - Test (15%): √öltimos 15% de datos

### Modelos

**Regresi√≥n Log√≠stica**:
- Regularizaci√≥n L2 (C=1.0)
- Solver: LBFGS
- Max iterations: 1000
- Random state: 42

**SVM-RBF**:
- Kernel: RBF (Radial Basis Function)
- C=1.0
- Gamma: 'scale'
- Probability: True (para obtener probabilidades)
- Random state: 42

### Protocolo Anti-Leakage

‚úÖ **Garant√≠as de no filtraci√≥n de informaci√≥n**:

1. **Split temporal**: Los datos de test son cronol√≥gicamente posteriores a train/val
2. **StandardScaler**: 
   - `fit()` solo en train
   - `transform()` en val y test
3. **Walk-forward validation**:
   - Cada fold solo usa datos pasados para entrenar
   - Nunca se usa informaci√≥n futura
4. **Sin tuning en test**: 
   - Test se eval√∫a UNA SOLA VEZ
   - No hay optimizaci√≥n de hiperpar√°metros en este proyecto

### M√©tricas

- **Accuracy**: Precisi√≥n global
- **Balanced Accuracy**: Promedio de recall por clase (importante para clases balanceadas)
- **F1-Score**: Media arm√≥nica de precision y recall
- **ROC-AUC**: √Årea bajo la curva ROC (capacidad de discriminaci√≥n)
- **Baseline**: M√°ximo entre clase mayoritaria (para comparaci√≥n)

## üêõ Troubleshooting

### Error: "Module 'pandas' not found"

**Soluci√≥n**: Aseg√∫rate de tener el entorno virtual activado e instalar dependencias:
```powershell
.\venv\Scripts\Activate.ps1
pip install -r requirements.txt
```

### Error: "Input X contains infinity"

**Soluci√≥n**: Este error ya fue corregido en `make_dataset.py`. Si persiste:
```powershell
# Regenerar el dataset
python core/data/make_dataset.py
```

### Error: "No such file or directory: datasets/nyse/"

**Soluci√≥n**: Aseg√∫rate de haber descargado y extra√≠do el dataset de Stooq en el directorio correcto.

### Proceso muy lento

**Para SVM**: El kernel RBF es computacionalmente intensivo con 10M+ registros. Esto es normal y puede tomar 20-30 minutos.

**Alternativa**: Si necesitas resultados m√°s r√°pidos para pruebas, puedes modificar `train_eval.py` temporalmente para usar solo un subset de datos:
```python
# En la funci√≥n main(), despu√©s de cargar el dataset:
df = df.sample(frac=0.1, random_state=42)  # Usar solo 10% de datos
```

### Problemas de memoria

**Soluci√≥n**: Si tienes < 8 GB de RAM, considera:
1. Cerrar otras aplicaciones
2. Usar el subset de datos mencionado arriba
3. Procesar un modelo a la vez (comentar uno en `train_eval.py`)

## üìö Referencias

- **Stooq Database**: https://stooq.com/db/h/
- **Scikit-learn Documentation**: https://scikit-learn.org/
- **Proyecto acad√©mico**: Secci√≥n 5.1 - Aplicaci√≥n al Modelo

## üë• Autores

Proyecto desarrollado para el curso de Aprendizaje Estad√≠stico.

## üìÑ Licencia

Este proyecto es para uso acad√©mico √∫nicamente.

---

**√öltima actualizaci√≥n**: Noviembre 2025

**Estado del proyecto**: ‚úÖ Dataset construido | üîÑ Entrenamiento en progreso
